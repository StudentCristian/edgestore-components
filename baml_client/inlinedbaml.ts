/*************************************************************************************************

Welcome to Baml! To use this generated code, please run one of the following:

$ npm install @boundaryml/baml
$ yarn add @boundaryml/baml
$ pnpm add @boundaryml/baml

*************************************************************************************************/

// This file was generated by BAML: please do not edit it. Instead, edit the
// BAML files and re-generate this code using: baml-cli generate
// You can install baml-cli with:
//  $ npm install @boundaryml/baml
//
/* eslint-disable */
// tslint:disable
// @ts-nocheck
// biome-ignore format: autogenerated code

const fileMap = {
  
  "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4oMini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet {\n  provider anthropic\n  options {\n    model \"claude-3-5-sonnet-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-haiku-20240307\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT4oMini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT4oMini, CustomGPT4oMini]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant { \n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}\n\nclient<llm> Gemini { \n  provider google-ai\n  options {\n    model \"gemini-2.0-flash\"\n    api_key env.GOOGLE_API_KEY\n    generationConfig {\n      max_output_tokens 8192          \n    }\n  }\n}\n",
  "document_generator.baml.baml": "// Defining a data model.\r\nclass DynamicFields {\r\n  @@dynamic \r\n}\r\n\r\n// Template actualizada para manejar contexto y prompts\r\ntemplate_string FormTemplate(context_data: string, fields_data: string) #\"\r\nEres un experto en la creación de contenido estructurado para documentos profesionales. Debes generar contenido EXTENSO, DETALLADO y COMPLETO para cada sección solicitada.\r\n\r\n{% if context_data %}\r\n=== INFORMACIÓN DE CONTEXTO ===\r\n{{ context_data }}\r\n\r\n{% endif %}\r\n=== INSTRUCCIONES DE GENERACIÓN ===\r\nPara cada campo solicitado, genera contenido EXTENSO siguiendo estas pautas:\r\n\r\n- MÍNIMO 3-4 párrafos por sección\r\n- Incluye ejemplos específicos y detallados\r\n- Desarrolla cada idea completamente\r\n- Usa subtemas y puntos clave\r\n- Proporciona explicaciones profundas\r\n\r\n{{ _.role('user') }}\r\n{{ fields_data }}\r\n\r\nIMPORTANTE: \r\n- NO generes respuestas cortas o superficiales\r\n- DESARROLLA completamente cada tema\r\n- INCLUYE detalles, ejemplos y explicaciones\r\n- ESTRUCTURA el contenido con subtemas cuando sea apropiado\r\n- Cada sección debe ser SUSTANCIAL y COMPLETA\r\n\r\n---\r\n{{ ctx.output_format }}\r\n\"#\r\n\r\n\r\n// Función actualizada que recibe contexto y prompts\r\nfunction ProcessForm(context_data: string, fields_data: string) -> DynamicFields {\r\n  client Gemini\r\n  prompt #\"\r\n    {{ FormTemplate(context_data, fields_data) }}\r\n  \"#\r\n}\r\n",
  "generators.baml": "generator typescript {\n  output_type \"typescript/react\"\n  output_dir \"../\"\n  version \"0.214.2\"\n}\n",
  "resume.baml": "// Defining a data model.\nclass Resume {\n  markdown string\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume(resume: string) -> Resume {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client Gemini  \n  prompt #\"\n    Adaptar a la sintaxis markdown vas a transcribir en el schema markdown \n    {{ resume }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest vaibhav_resume {\n  functions [ExtractResume]\n  args {\n    resume #\"\n      # This is header for diagram below\n\n      ```mermaid\n        graph TD;\n            A-->B;\n            A-->C;\n            B-->D;\n            C-->D;\n      ```\n    \"#\n  }\n}\n",
}
export const getBamlFiles = () => {
    return fileMap;
}